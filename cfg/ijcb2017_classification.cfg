[net]
# how many images are in each batch to average the loss over?
batch=64
# into how many sub-batches shall each batch be divided to handle
# images in each sub-batch in parallel?
subdivisions=8
# input size of the network
height=448
width=448
max_crop=512
# height=224
# width=224
# max_crop=320
# colour space
channels=3
# learning parameters
momentum=0.9
decay=0.0005

# # randomly adjust the exposure and saturation of the
# # image by up to a factor of 1.5 in the HSV color space
# angle=7
# hue = .1
# saturation=.75
# exposure=.75
# aspect=.75

# max number of iterations
max_batches = 1091700 # 10917 training images x 100 epochs 

# base learning rate
learning_rate=0.001

# change learning rate after the corresponding steps
policy=poly
power=4

# # change learning rate after the corresponding steps
# policy=steps
# steps=100000,600000,800000
# # re-scale the current learning rate by the correponding factor once
# # the number of steps is reached
# scales=10,1,.1

# snapshow the learned weights after every k "iterations"
i_snapshot_iteration=50

# Implemented Activation Functions
# linear
# logistic
# loggy
# relu
# elu
# relie
# ramp
# leaky
# tanh
# plse
# stair
# hardtan
# lhtan

################################################################################

# [convolutional]
# batch_normalize=1
# filters=32
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=64
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=64
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=128
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=256
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=1024
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=1024
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=1024
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# filters=1086
# size=1
# stride=1
# pad=1
# activation=linear

# [avgpool]

# [softmax]
# groups=1

# [cost]
# type=sse

################################################################################

# [convolutional]
# batch_normalize=1
# filters=64
# size=7
# stride=2
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=192
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=128
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=256
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=512
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=1024
# size=3
# stride=1
# pad=1
# activation=leaky

# [maxpool]
# size=2
# stride=2

# [convolutional]
# batch_normalize=1
# filters=1024
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=2048
# size=3
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=1024
# size=1
# stride=1
# pad=1
# activation=leaky

# [convolutional]
# batch_normalize=1
# filters=2048
# size=3
# stride=1
# pad=1
# activation=leaky

# [avgpool]

# [connected]
# output=1086
# activation=leaky

# [softmax]
# groups=1

# [cost]
# type=sse

################################################################################

[convolutional]
batch_normalize=1
filters=32
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=64
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=3
stride=1
pad=1
activation=leaky

[maxpool]
size=2
stride=2

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
filters=1024
size=3
stride=1
pad=1
activation=leaky


#######

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[route]
layers=-9

[reorg]
stride=2

[route]
layers=-1,-3

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=1024
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=5450
activation=linear

[region]
anchors = 1.08,1.19,  3.42,4.41,  6.63,11.38,  9.42,5.11,  16.62,10.52
bias_match=1
classes=1085
# bounding boxes -> 4 parameters (l.n)
coords=4
# S -- number of grid cells in x and y direction
side=14 # double the number from original paper
# B -- number of predicted bounding boxes per grid cell
num=5
softmax=1
# data augmentation in the form of random scaling and
# translations of up to 20% of the original image size.
jitter=.2
rescore=1

# Relative weights for the combined loss function
object_scale=1
# \lambda_{noobj} -- parameter manipulating loss from confidence predictions
# for boxes that donâ€™t contain objects
noobject_scale=.5
class_scale=1
# \lambda_{coord} -- parameter manipulating loss from bounding box
# coordinate predictions
coord_scale=5

absolute=1
thresh = .06
random=1
